{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd86c26",
   "metadata": {},
   "source": [
    "# Task 2: ETL Process Implementation\n",
    "\n",
    "This notebook implements an end‑to‑end ETL pipeline for the Online Retail dataset into a simple SQLite star schema consisting of a SalesFact table and two dimensions: CustomerDim and TimeDim. Each section is clearly labeled to align with rubric requirements.\n",
    "\n",
    "We now use ONLY the provided Online Retail Excel file (`raw_data/Online Retail.xlsx`). Synthetic data generation has been removed for clarity and reproducibility.\n",
    "\n",
    "Outline of Steps (Sections 1–20 below): Imports, Parameters, Excel Load, Profiling, Cleaning, Transformations, Dimension Builds, Fact Prep, SQLite Load, Orchestration Function, Logging & Validation, and Sanity Queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b57ae",
   "metadata": {},
   "source": [
    "# Task 2: ETL Process (Modular Version)\n",
    "\n",
    "\n",
    "This notebook now calls functions from `utils/etl.py` for all core logic so it remains minimal and modular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b1028",
   "metadata": {},
   "source": [
    "## Why these parameters?\n",
    "\n",
    "- FIXED_CURRENT_DATE anchors a stable time window for grading/reproducibility.\n",
    "- We detect the project root so imports (`utils/etl.py`) work regardless of notebook location.\n",
    "- DB_PATH points to `data_warehouse/retail_dw.db` so code/scripts share the same output.\n",
    "\n",
    "Run the next cell to execute the full ETL and print table counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28a26bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ROOT: K:\\Code Projects\\DSA2040_Practical_Exam_Justice_444\n",
      "Excel: K:\\Code Projects\\DSA2040_Practical_Exam_Justice_444\\raw_data\\Online Retail.xlsx\n",
      "DB: K:\\Code Projects\\DSA2040_Practical_Exam_Justice_444\\data_warehouse\\retail_dw.db\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and Configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Robust project root detection for notebooks (no __file__ in Jupyter)\n",
    "# Strategy:\n",
    "# 1) If the workspace folder name is present in CWD path, use it as root.\n",
    "# 2) Otherwise walk up until we find markers like 'utils' folder or 'raw_data' folder.\n",
    "# 3) Finally, fall back to the notebook directory.\n",
    "\n",
    "CWD = Path.cwd().resolve()\n",
    "markers = [\"utils\", \"raw_data\", \".git\", \"data_mining\", \"data_warehouse\"]\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(10):  # prevent infinite loops\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return start\n",
    "\n",
    "ROOT = find_project_root(CWD)\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from utils.etl import run_etl\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "\n",
    "# Parameters\n",
    "FIXED_CURRENT_DATE = pd.Timestamp('2025-08-12')\n",
    "DATA_DIR = (ROOT / 'raw_data').resolve()\n",
    "EXCEL_PATH = DATA_DIR / 'Online Retail.xlsx'\n",
    "DB_PATH = (ROOT / 'data_warehouse' / 'retail_dw.db').resolve()\n",
    "\n",
    "print('Project ROOT:', ROOT)\n",
    "print('Excel:', EXCEL_PATH)\n",
    "print('DB:', DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f9c5c",
   "metadata": {},
   "source": [
    "## What the ETL does\n",
    "\n",
    "1. Extract from Excel (`raw_data/Online Retail.xlsx`).\n",
    "2. Clean types, drop invalid rows, add `TotalSales`.\n",
    "3. Select a 12-month window (or fallback if data is older).\n",
    "4. Build dimensions: Customer, Product (with simple Category), and Time.\n",
    "5. Build SalesFact with foreign keys to all three dims.\n",
    "6. Create the SQLite schema and load the tables + indexes.\n",
    "\n",
    "Outputs a count summary to confirm row volumes at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed9c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] ETL complete: {'raw': 541909, 'cleaned': 530104, 'last_year': 509238, 'customers': 4273, 'products': 4112, 'dates': 297, 'fact': 384512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count Summary: {'raw': 541909, 'cleaned': 530104, 'last_year': 509238, 'customers': 4273, 'products': 4112, 'dates': 297, 'fact': 384512}\n"
     ]
    }
   ],
   "source": [
    "# 2. Run ETL\n",
    "counts = run_etl(EXCEL_PATH, DB_PATH, FIXED_CURRENT_DATE)\n",
    "print('Row Count Summary:', counts)\n",
    "assert counts['customers'] > 0 and counts['products'] > 0 and counts['dates'] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058320db",
   "metadata": {},
   "source": [
    "## Validation notes\n",
    "\n",
    "- Expect non-zero counts for customers, products, and dates.\n",
    "- If an error mentions missing Excel, ensure `raw_data/Online Retail.xlsx` exists.\n",
    "- If DB write issues occur, the process recreates tables each run—no manual cleanup needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
