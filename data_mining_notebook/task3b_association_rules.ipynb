{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7f3312",
   "metadata": {},
   "source": [
    "# Section 2: Data Mining\n",
    "## Task 3 Part B: Association Rule Mining (10 Marks)\n",
    "This stand-alone notebook generates synthetic transactional data, runs Apriori (mlxtend if available, else a minimal fallback), extracts rules with specified thresholds, and provides analytical commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81428cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random, json\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "ARTIFACT_DIR = Path('artifacts')\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "print('Configuration initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Synthetic Transaction Generation\n",
    "item_pool = ['milk','bread','butter','cheese','eggs','beer','diapers','apples','bananas','cereal',\n",
    "              'chicken','rice','pasta','tomatoes','onions','yogurt','chips','soda','coffee','tea']\n",
    "n_transactions = 45  # within required 20-50 range\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "transactions = []\n",
    "for _ in range(n_transactions):\n",
    "    basket_size = rng.integers(3,9)\n",
    "    basket = rng.choice(item_pool, size=basket_size, replace=False).tolist()\n",
    "    # Inject patterned co-occurrences\n",
    "    if rng.random() < 0.45:  # bread & milk synergy\n",
    "        for it in ['milk','bread']:\n",
    "            if it not in basket: basket.append(it)\n",
    "    if rng.random() < 0.30:  # beer & diapers classic example\n",
    "        for it in ['beer','diapers']:\n",
    "            if it not in basket: basket.append(it)\n",
    "    if rng.random() < 0.35:  # coffee & tea pairing\n",
    "        for it in ['coffee','tea']:\n",
    "            if it not in basket: basket.append(it)\n",
    "    transactions.append(sorted(set(basket)))\n",
    "print('First 5 transactions sample:')\n",
    "for t in transactions[:5]:\n",
    "    print(t)\n",
    "print('Total transactions:', len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apriori Mining (mlxtend preferred, fallback otherwise)\n",
    "MIN_SUPPORT = 0.2\n",
    "MIN_CONF = 0.5\n",
    "try:\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    use_mlxtend = True\n",
    "    print('Using mlxtend Apriori implementation.')\n",
    "except ImportError:\n",
    "    use_mlxtend = False\n",
    "    print('mlxtend not installed; using simplified fallback Apriori (pairs only).')\n",
    "\n",
    "if use_mlxtend:\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df_tx = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    frequent = apriori(df_tx, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "    rules = association_rules(frequent, metric='confidence', min_threshold=MIN_CONF)\n",
    "    rules = rules[['antecedents','consequents','support','confidence','lift','leverage','conviction']]\n",
    "else:\n",
    "    # Minimal fallback focusing on singletons & pairs\n",
    "    def support(itemset):\n",
    "        count = sum(1 for t in transactions if set(itemset).issubset(t))\n",
    "        return count / len(transactions)\n",
    "    singles = sorted({i for t in transactions for i in t})\n",
    "    L1 = [{i} for i in singles if support({i}) >= MIN_SUPPORT]\n",
    "    pairs = []\n",
    "    for i in range(len(L1)):\n",
    "        for j in range(i+1, len(L1)):\n",
    "            cand = L1[i] | L1[j]\n",
    "            sup = support(cand)\n",
    "            if sup >= MIN_SUPPORT:\n",
    "                pairs.append((cand, sup))\n",
    "    rows = []\n",
    "    for cand, sup in pairs:\n",
    "        a,b = tuple(cand)\n",
    "        sup_a, sup_b = support({a}), support({b})\n",
    "        conf_a_b = sup / sup_a if sup_a else 0\n",
    "        conf_b_a = sup / sup_b if sup_b else 0\n",
    "        lift_a_b = conf_a_b / sup_b if sup_b else 0\n",
    "        lift_b_a = conf_b_a / sup_a if sup_a else 0\n",
    "        if conf_a_b >= MIN_CONF:\n",
    "            rows.append({'antecedents': {a}, 'consequents': {b}, 'support': sup, 'confidence': conf_a_b, 'lift': lift_a_b})\n",
    "        if conf_b_a >= MIN_CONF:\n",
    "            rows.append({'antecedents': {b}, 'consequents': {a}, 'support': sup, 'confidence': conf_b_a, 'lift': lift_b_a})\n",
    "    rules = pd.DataFrame(rows)\n",
    "    if not rules.empty: rules['leverage'] = np.nan; rules['conviction'] = np.nan\n",
    "\n",
    "if rules.empty:\n",
    "    print('No rules found at given thresholds. Consider lowering support/confidence.')\n",
    "else:\n",
    "    print('Total candidate rules:', len(rules))\n",
    "rules_sorted = rules.sort_values('lift', ascending=False).head(5).reset_index(drop=True)\n",
    "display(rules_sorted)\n",
    "rules_sorted.to_csv(ARTIFACT_DIR / 'top5_rules_partB.csv', index=False)\n",
    "print('Saved top 5 rules to artifacts/top5_rules_partB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7193439",
   "metadata": {},
   "source": [
    "# 4. Analysis\n",
    "A representative high-lift rule such as {bread} → {milk} implies that the presence of bread meaningfully increases the probability that milk appears in the same basket relative to baseline frequency. Practically, retailers can exploit this by: (1) cross-promoting milk near bread aisles, (2) bundling discounts to increase average basket value, and (3) ensuring synchronized replenishment to avoid stockouts that would reduce rule utility. Lift’s normalization over marginal supports helps filter out spurious popularity-driven associations. Still, rules must be validated over time: seasonality, promotions, and changing customer habits can erode rule strength. A/B testing recommendations based on the rule (e.g., suggesting milk at online checkout after bread is added) quantifies uplift in conversion. Additionally, combining rules with customer segmentation may personalize which associations to prioritize for distinct shopper cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Metadata & Persistence\n",
    "metadata = {\n",
    "    'n_transactions': len(transactions),\n",
    "    'min_support': MIN_SUPPORT,\n",
    "    'min_confidence': MIN_CONF,\n",
    "    'rules_found': int(len(rules)),\n",
    "    'top5_rules_count': int(len(rules_sorted))\n",
    "}\n",
    "with open(ARTIFACT_DIR / 'task3b_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print('Saved metadata to artifacts/task3b_metadata.json')\n",
    "display(pd.DataFrame([metadata]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6b615",
   "metadata": {},
   "source": [
    "### Part B Complete\n",
    "Generated transactions, mined rules with Apriori (or fallback), exported top 5 by lift, and provided actionable analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
